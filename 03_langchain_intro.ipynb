{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea",
   "metadata": {
    "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976",
   "metadata": {
    "id": "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 3:** LangChain Expression Language</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we introduced some of the services we'll be taking advantage of for our LLM applications, including some external LLM platforms and a locally-hosted front-end service. Both of these components incorporate LangChain, but it hasn't been spotlighted as a major focus yet. It is expected that you have some experience with LangChain and LLMs, but this notebook is intended catch you up to the level necessary to progress through the rest of the course!\n",
    "\n",
    "This notebook is designed to guide you through the integration and application of LangChain, a leading orchestration library for Large Language Models (LLMs), with the AI Foundation Endpoints from last time. Whether you are a seasoned developer or new to LLMs, this course will enhance your understanding and skills in building sophisticated LLM applications.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learning how to leverage chains and runnables to orchestrate interesting LLM systems.  \n",
    "- Getting familiar with using LLMs for external conversation and internal reasoning.\n",
    "- Be able to start up and run a simple Gradio interface inside your notebook.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- What kinds of utilities might be necessary to keep information flowing through the pipeline **(primer for next notebook)**.\n",
    "- When you encounter the `gradio` interface, consider where all you have seen this style of interface before. Some possible places may include [HuggingFace Spaces](https://huggingface.co/spaces), but did you know that the NGC playground environments for foundation models use it? Hypothesize what kinds of options `gradio` might allow to make this level of flexibility possible?\n",
    "- Near the end of the section, you'll learn that you can pass around chains as routes and access them across environments via ports. What kinds of requirements should you advertise if you are trying to recieve chains from other microservices?\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://learn.next.courses.nvidia.com/courses/course-v1:DLI+S-FX-15+V1/about). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zUaZXR75v-jD",
   "metadata": {
    "id": "zUaZXR75v-jD"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio typing_extensions>=4.8.0\n",
    "\n",
    "## If you're in colab and encounter a typing-extensions issue,\n",
    "##  restart your runtime and try again\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i-ZOOmvbybyE",
   "metadata": {
    "id": "i-ZOOmvbybyE"
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import requests\n",
    "import os\n",
    "\n",
    "hard_reset = False  ## <-- Set to True if you want to reset your NVIDIA_API_KEY\n",
    "while \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\") or hard_reset:\n",
    "    try: \n",
    "        assert not hard_reset\n",
    "        response = requests.get(\"http://docker_router:8070/get_key\").json()\n",
    "        assert response.get('nvapi_key')\n",
    "    except: response = {'nvapi_key' : getpass(\"NVIDIA API Key: \")}\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = response.get(\"nvapi_key\")\n",
    "    try: requests.post(\"http://docker_router:8070/set_key/\", json={'nvapi_key' : os.environ[\"NVIDIA_API_KEY\"]}).json()\n",
    "    except: pass\n",
    "    hard_reset = False\n",
    "    if \"nvapi-\" not in os.environ.get(\"NVIDIA_API_KEY\", \"\"):\n",
    "        print(\"[!] API key assignment failed. Make sure it starts with `nvapi-` as generated from the model pages.\")\n",
    "\n",
    "print(f\"Retrieved NVIDIA_API_KEY beginning with \\\"{os.environ.get('NVIDIA_API_KEY')[:9]}...\\\"\")\n",
    "from langchain_nvidia_ai_endpoints._common import NVEModel\n",
    "NVEModel().available_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d427c-68e7-45ca-90f7-8799aa9d7eff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### **Considering Your Models**\n",
    "\n",
    "Going back to the [**NGC Catalog**](https://catalog.ngc.nvidia.com/ai-foundation-models), we'll be able to find a selection of interesting models that you can invoke from your environment. These models are all there because there is valid use for them in production pipelines, so it's a good idea to look around and find out which models are best for your use cases.\n",
    "\n",
    "**The code provided includes some models already listed, but you may want (or need) to upgrade to other models if you notice a strictly-better option or a model is no longer available. *This comment will apply throughout the rest of the course, so keep that in mind!***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jhAKeBiDurIz",
   "metadata": {
    "id": "jhAKeBiDurIz"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** What Is LangChain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "evFkwVD6ux-B",
   "metadata": {
    "id": "evFkwVD6ux-B"
   },
   "source": [
    "LangChain is an popular LLM orchestration library to help set up systems that have one or more LLM components. The library is, for better or worse, extremely popular and changes rapidly based on new developments in the field, meaning that somebody can have a lot of experience in some parts of LangChain while having little-to-no familiarity with other parts (either because there are just so many different features or the area is new and the features have only recently been implemented).\n",
    "\n",
    "This notebook will be using the **LangChain Expression Language (LCEL)** to ramp up from basic chain specification to more advanced dialog management practices, so hopefully the journey will be enjoyable and even seasoned LangChain developers might learn something new!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf",
   "metadata": {
    "id": "a3ced9b0-30ed-4ccc-936f-ca03d6e172bf"
   },
   "source": [
    "<!-- > <img style=\"max-width: 400px;\" src=\"imgs/langchain-diagram.png\" /> -->\n",
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/langchain-diagram.png\" width=400px/>\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1NS7dmLf5ql04o5CyPZnd1gnXXgO8-jbR\" width=400px/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff906c9-d776-4117-b4f6-19ff535b8f57",
   "metadata": {
    "id": "3ff906c9-d776-4117-b4f6-19ff535b8f57"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Chains and Runnables\n",
    "\n",
    "When exploring a new library, it's important to note what are the core systems of the library and how are they used.\n",
    "\n",
    "In LangChain, the main building block *used to be* the classic **Chain**: a small module of functionality that does something specific and can be linked up with other chains to make a system. So for all intents and purposes, it is a \"building-block system\" abstraction where the building blocks are easy to create, have consistent methods (`invoke`, `generate`, `stream`, etc), and can be linked up to work together as a system. Some example legacy chains include `LLMChain`, `ConversationChain`, `TransformationChain`, `SequentialChain`, etc.\n",
    "\n",
    "More recently, a new recommended specification has emerged that is significantly easier to work with and extremely compact, the **LangChain Expression Language (LCEL)**. This new format relies on a different kind of primitive - a **Runnable** - which is simply an object that wraps a function. Allow dictionaries to be implicitly converted to Runnables and let a **pipe |** operator create a Runnable that passes data from the left to the right (i.e. `fn1 | fn2` is a Runnable), and you have a simple way to specify complex logic!\n",
    "\n",
    "Here are some very representative example Runnables, created via the `RunnableLambda` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e676de75-cc9b-4fa2-8ce7-d3bd6f9949b7",
   "metadata": {
    "id": "e676de75-cc9b-4fa2-8ce7-d3bd6f9949b7"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from functools import partial\n",
    "\n",
    "################################################################################\n",
    "## Very simple \"take input and return it\"\n",
    "identity = RunnableLambda(lambda x: x)  ## Or RunnablePassthrough works\n",
    "\n",
    "################################################################################\n",
    "## Given an arbitrary function, you can make a runnable with it\n",
    "def print_and_return(x, preface=\"\"):\n",
    "    print(f\"{preface}{x}\")\n",
    "    return x\n",
    "\n",
    "rprint0 = RunnableLambda(print_and_return)\n",
    "\n",
    "################################################################################\n",
    "## You can also pre-fill some of values using functools.partial\n",
    "rprint1 = RunnableLambda(partial(print_and_return, preface=\"1: \"))\n",
    "\n",
    "################################################################################\n",
    "## And you can use the same idea to make your own custom Runnable generator\n",
    "def RPrint(preface=\"\"):\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "################################################################################\n",
    "## Chaining two runnables\n",
    "chain1 = identity | rprint0\n",
    "chain1.invoke(\"Hello World!\")\n",
    "print()\n",
    "\n",
    "################################################################################\n",
    "## Chaining that one in as well\n",
    "output = (\n",
    "    chain1\n",
    "    | rprint1\n",
    "    | RPrint(\"2: \")\n",
    ").invoke(\"Welcome Home!\")\n",
    "\n",
    "print(\"\\nOutput:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb670bea-4593-49a2-9b58-6ff2becf1dbe",
   "metadata": {
    "id": "cb670bea-4593-49a2-9b58-6ff2becf1dbe"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3:** Dictionary Pipelines with Chat Models\n",
    "\n",
    "There's a lot you can do with runnables, but it's important to formalize some best practices based on use case and current syntax. At the moment, it's easiest to use *dictionaries* as our default variable containers for a few key reasons\" \n",
    "\n",
    "**Passing dictionaries helps us keep track of our variables by name.**\n",
    "\n",
    "Since dictionaries allow us to propagate named variables (values referenced by keys), using them is great for locking in our chain components' outputs and expectations.\n",
    "\n",
    "**LangChain prompts expect dictionaries of values.**\n",
    "\n",
    "It's quite intuitive to specify an LLM Chain in LCEL to take in a dictionary and produce a string, and equally easy to raise said string back up to be a dictionary. This is very intentional and is partially due to the above reason. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xMHAThLp_AgQ",
   "metadata": {
    "id": "xMHAThLp_AgQ"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Example 1:** A Simple LLM Chain\n",
    "\n",
    "One of the most fundamental components of classical LangChain is the `LLMChain` that accepts a **prompt** and an **LLM**:\n",
    "\n",
    "- A prompt, usually retrieved from a call like `PromptTemplate.from_template(\"string with {key1} and {key2}\")`, specifies a template for creating a string as output. A dictionary `{\"key1\" : 1, \"key2\" : 2}` could be passed in to get the output `\"string with 1 and 2\"`.\n",
    "    - For chat models like `ChatNVIDIA`, you would use `ChatPromptTemplate.from_messages` instead.\n",
    "- An LLM takes in a string and returns a generated string.\n",
    "    - Chat models like `ChatNVIDIA` deal in messages instead, but it's the same idea! Using an **StrOutputParser** at the end will extract the content from the message.\n",
    "\n",
    "The following is a lightweight example of a simple chat chain as described above. All it does is take in an input dictionary and use it fill in a system message to specify the overall meta-objective and a user input to specify query the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uYBqZ_Za985q",
   "metadata": {
    "id": "uYBqZ_Za985q"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## Simple Chat Pipeline\n",
    "chat_llm = ChatNVIDIA(model=\"llama2_13b\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Only respond in rhymes\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "rhyme_chain = prompt | chat_llm | StrOutputParser()\n",
    "\n",
    "print(rhyme_chain.invoke({\"input\" : \"Tell me about birds!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4torS7DgBk2T",
   "metadata": {
    "id": "4torS7DgBk2T"
   },
   "source": [
    "<br>\n",
    "\n",
    "In addition to just using the code command as-is, we can try using a [gradio interface](https://www.gradio.app/guides/creating-a-chatbot-fast) to play around with our model. Gradio is a popular tool that provides simple building blocks for creating custom generative AI interfaces! The below example shows how you can make an easy gradio chat interface with this particular example chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CAQ_DjX7-2oO",
   "metadata": {
    "id": "CAQ_DjX7-2oO"
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "#######################################################\n",
    "## Non-streaming Interface like that shown above\n",
    "\n",
    "# def rhyme_chat(message, history):\n",
    "#     return rhyme_chain.invoke({\"input\" : message})\n",
    "\n",
    "# gr.ChatInterface(rhyme_chat).launch()\n",
    "\n",
    "#######################################################\n",
    "## Streaming Interface\n",
    "\n",
    "def rhyme_chat_stream(message, history):\n",
    "    ## This is a generator function, where each call will yield the next entry\n",
    "    buffer = \"\"\n",
    "    for token in rhyme_chain.stream({\"input\" : message}):\n",
    "        buffer += token\n",
    "        yield buffer\n",
    "\n",
    "## Uncomment when you're ready to try this. IF USING COLAB: Share=False is faster\n",
    "gr.ChatInterface(rhyme_chat_stream).queue().launch(share=True, debug=True) \n",
    "\n",
    "## NOTE: When you're done, please click the Square button (twice to be safe) to stop the session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tBGUORsV96_E",
   "metadata": {
    "id": "tBGUORsV96_E"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Example 2: Internal Response**\n",
    "\n",
    "Sometimes, you also want to have some quick reasoning that goes on behind the scenes before your response actually comes out to the user. When performing this task, you need a model with a strong instruction-following prior assumption built-in.\n",
    "\n",
    "The following is an example \"zero-shot classification\" pipeline which will try to categorize a sentence into one of a couple of classes.\n",
    "\n",
    "**In order, this zero-shot classification chain:**\n",
    "- Takes in a dictionary with two required keys, `input` and `options`.\n",
    "- Passes it through the zero-shot prompt to get the input to our LLM.\n",
    "- Passes that string to the model to get the result.\n",
    "\n",
    "**Task:** Pick out several models that you think would be good for this kind of task and see how well they perform! Specifically:\n",
    "- **Try to find models that are predictable across multiple examples.** If the format is always easy to parse and extremely predictable, then the model is probably ok.\n",
    "- **Try to find models that are also fast!** This is important because internal reasoning generally happens behind the hood before the external response gets generated. Thereby, it is a blocking process which can slow down start of \"user-facing\" generation, making your system feel sluggish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b1868d-4ece-4ee6-b6c3-92ec7164bda5",
   "metadata": {
    "id": "39b1868d-4ece-4ee6-b6c3-92ec7164bda5"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "## TODO: Try out some more models and see if there are better options\n",
    "instruct_llm = ChatNVIDIA(model=\"llama2_13b\")\n",
    "\n",
    "## Zero-shot classification prompt and chain\n",
    "zsc_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"Pick the most likely topic of the sentence. Choose an one option of the following: {options}. Only one word-answers\"\n",
    "    )),\n",
    "    (\"user\", \"[Options : {options}] {input} = \")\n",
    "])\n",
    "\n",
    "zsc_chain = zsc_prompt | instruct_llm | StrOutputParser()\n",
    "\n",
    "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
    "    return zsc_chain.invoke({\"input\" : input, \"options\" : options})\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jXdjidkkKG9W",
   "metadata": {
    "id": "jXdjidkkKG9W"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Example 3: Multi-Component Chains**\n",
    "\n",
    "The previous example showed how we can coerce a dictionary into a string by passing it through a prompt->LLM chain, so that's one easy structure to motivate the container choice. But is it just as easy to convert the string output back up to a dictionary?\n",
    "\n",
    "**Yes, it is!** The simplest way is actually to use the LCEL *\"implicit runnable\"* syntax, which allows you to use a dictionary of functions (including chains) as a runnable that runs each function and maps the value to the key in the output dictionary.\n",
    "\n",
    "The following is an example which exercises these utilities while also providing a few extra tools you may find useful in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yi8-lKSIKhXe",
   "metadata": {
    "id": "Yi8-lKSIKhXe"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Example of dictionary enforcement methods\n",
    "def make_dictionary(v, key):\n",
    "    if isinstance(v, dict):\n",
    "        return v\n",
    "    return {key : v}\n",
    "\n",
    "def RInput(key='input'):\n",
    "    '''Coercing method to mold a value (i.e. string) to in-like dict'''\n",
    "    return RunnableLambda(partial(make_dictionary, key=key))\n",
    "\n",
    "def ROutput(key='output'):\n",
    "    '''Coercing method to mold a value (i.e. string) to out-like dict'''\n",
    "    return RunnableLambda(partial(make_dictionary, key=key))\n",
    "\n",
    "################################################################################\n",
    "## Common LCEL utility for pulling values from dictionaries\n",
    "from operator import itemgetter\n",
    "\n",
    "up_and_down = (\n",
    "    RPrint(\"A: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | RInput()\n",
    "    | RPrint(\"B: \")\n",
    "    ## Pull-values-from-dictionary utility\n",
    "    | itemgetter(\"input\")\n",
    "    | RPrint(\"C: \")\n",
    "    ## Anything-in Dictionary-out implicit map\n",
    "    | {\n",
    "        'word1' : (lambda x : x.split()[0]),\n",
    "        'word2' : (lambda x : x.split()[1]),\n",
    "        'words' : (lambda x: x),  ## <- == to RunnablePassthrough()\n",
    "    }\n",
    "    | RPrint(\"D: \")\n",
    "    | itemgetter(\"word1\")\n",
    "    | RPrint(\"E: \")\n",
    "    ## Anything-in anything-out lambda application\n",
    "    | RunnableLambda(lambda x: x.upper())\n",
    "    | RPrint(\"F: \")\n",
    "    ## Custom ensure-dictionary process\n",
    "    | ROutput()\n",
    ")\n",
    "\n",
    "up_and_down.invoke({\"input\" : \"Hello World\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kk0t7kUcMhFT",
   "metadata": {
    "id": "kk0t7kUcMhFT"
   },
   "outputs": [],
   "source": [
    "## NOTE how the dictionary enforcement methods make it easy to make the following syntax equivalent\n",
    "up_and_down.invoke(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LVIagnD0byq1",
   "metadata": {
    "id": "LVIagnD0byq1"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Rhyme Re-themer Chatbot\n",
    "\n",
    "Below is a poetry generation example that showcases how you might organize two different tasks under the guise of a single agent. The system calls back to the simple Gradio example, but extends it with some boiler-plate responses and logic behind the scenes.\n",
    "\n",
    "It's primary feature is as follows:\n",
    "- On the first response, it will generate a poem based on your response.\n",
    "- On subsequent responses, it will keep the format and structure of your original rhyme while modifying the topic of the poem.\n",
    "\n",
    "**Problem:** At present, the system should function just fine for the first part, but the second part is not yet implemented.\n",
    "\n",
    "**Objective:** Implement the rest of the `rhyme_chat2_stream` method such that the agent is able to function normally.\n",
    "\n",
    "To make the gradio component easier to reason with, a simplified `queue_fake_streaming_gradio` method is provided that will simulate the gradio chat event loop with the standard Python `input` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B-bzsMyrKQ5m",
   "metadata": {
    "id": "B-bzsMyrKQ5m"
   },
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from copy import deepcopy\n",
    "\n",
    "inst_llm = ChatNVIDIA(model=\"mixtral_8x7b\")  ## Feel free to change the models\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Only respond in rhymes\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "prompt2 =  ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"Only responding in rhyme, change the topic of the input poem to be about {topic}!\"\n",
    "        \" Make it happy! Try to keep the same sentence structure, but make sure it's easy to recite!\"\n",
    "        \" Try not to rhyme a word with itself.\"\n",
    "    )),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "## These are the main chains, constructed here as modules of functionality.\n",
    "chain1 = prompt1 | inst_llm | StrOutputParser()  ## only expects input\n",
    "chain2 = prompt2 | inst_llm | StrOutputParser()  ## expects both input and topic\n",
    "\n",
    "################################################################################\n",
    "## SUMMARY OF TASK: chain1 currently gets invoked for the first input.\n",
    "##  Please invoke chain2 for subsequent invocations.\n",
    "\n",
    "def rhyme_chat2_stream(message, history, return_buffer=True):\n",
    "    '''This is a generator function, where each call will yield the next entry'''\n",
    "\n",
    "    first_poem = None\n",
    "    for entry in history:\n",
    "        if entry[0] and entry[1]:\n",
    "            ## If a generation occurred as a direct result of a user input,\n",
    "            ##  keep that response (the first poem generated) and break out\n",
    "            first_poem = entry[1]\n",
    "            break\n",
    "\n",
    "    if first_poem is None:\n",
    "        ## First Case: There is no initial poem generated. Better make one up!\n",
    "\n",
    "        buffer = \"Oh! I can make a wonderful poem about that! Let me think!\\n\\n\"\n",
    "        yield buffer\n",
    "\n",
    "        ## iterate over stream generator for first generation\n",
    "        inst_out = \"\"\n",
    "        chat_gen = chain1.stream({\"input\" : message})\n",
    "        for token in chat_gen:\n",
    "            inst_out += token\n",
    "            buffer += token\n",
    "            yield buffer if return_buffer else token\n",
    "\n",
    "        passage = \"\\n\\nNow let me rewrite it with a different focus! What should the new focus be?\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "    else:\n",
    "        ## Subsequent Cases: There is a poem to start with. Generate a similar one with a new topic!\n",
    "\n",
    "        buffer = f\"Sure! Here you go!\\n\\n\"\n",
    "        yield buffer\n",
    "\n",
    "        return  ## <- TODO: Early termination for generators. Comment this out\n",
    "\n",
    "        ########################################################################\n",
    "        ## TODO: Invoke the second chain to generate the new rhymes.\n",
    "\n",
    "        ## iterate over stream generator for second generation\n",
    "\n",
    "        ## END TODO\n",
    "        ########################################################################\n",
    "\n",
    "        passage = \"\\n\\nThis is fun! Give me another topic!\"\n",
    "        buffer += passage\n",
    "        yield buffer if return_buffer else passage\n",
    "\n",
    "################################################################################\n",
    "## Below: This is a small-scale simulation of the gradio routine.\n",
    "\n",
    "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=5):\n",
    "\n",
    "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
    "    for human_msg, agent_msg in history:\n",
    "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
    "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
    "\n",
    "    ## Mimic of the gradio loop with an initial message from the agent.\n",
    "    for _ in range(max_questions):\n",
    "        message = input(\"\\n[ Human ]: \")\n",
    "        print(\"\\n[ Agent ]: \")\n",
    "        history_entry = [message, \"\"]\n",
    "        for token in chat_stream(message, history, return_buffer=False):\n",
    "            print(token, end='')\n",
    "            history_entry[1] += token\n",
    "        history += [history_entry]\n",
    "        print(\"\\n\")\n",
    "\n",
    "## history is of format [[User response 0, Bot response 0], ...]\n",
    "history = [[None, \"Let me help you make a poem! What would you like for me to write?\"]]\n",
    "\n",
    "## Simulating the queueing of a streaming gradio interface, using python input\n",
    "queue_fake_streaming_gradio(\n",
    "    chat_stream = rhyme_chat2_stream,\n",
    "    history = history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k9-1X9EVQ42t",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "k9-1X9EVQ42t"
   },
   "outputs": [],
   "source": [
    "## Simple way to initialize history for the ChatInterface\n",
    "chatbot = gr.Chatbot(value = [[None, \"Let me help you make a poem! What would you like for me to write?\"]])\n",
    "\n",
    "## IF USING COLAB: Share=False is faster\n",
    "gr.ChatInterface(rhyme_chat2_stream, chatbot=chatbot).queue().launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VJ6mEQvgkH7w",
   "metadata": {
    "id": "VJ6mEQvgkH7w"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5: [Exercise]** Using Deeper LangChain Integrations\n",
    "\n",
    "This exercise that gives you an opportunity to investigate some example code regarding [**LangServe**](https://www.langchain.com/langserve). Specifically, we refer to the [**`frontend`**](frontend) directory as well as the [**`35_langserve.ipynb`**](35_langserve.ipynb) notebook.\n",
    "\n",
    "This exercise **does** require the course environment, but feel free to come back to it when you're ready. This service will only be necessary for your final submission!\n",
    "\n",
    "- Visit [**`35_langserve.ipynb`**](35_langserve.ipynb) and run the provided script to start up a server with several active routes.\n",
    "- Once done, verify that the following [**LangServe `RemoteRunnable`**](https://python.langchain.com/docs/langserve) works. The goal of a [**`RemoteRunnable`**](https://python.langchain.com/docs/langserve) is to make it easy to host a LangChain chain as an API endpoint, so the following is just a test to make sure that it works.\n",
    "    - If it doesn't work the first time, there may be an order-of-operations issue. Feel free to try and restart the langserve notebook. LangServe is still rather early in its life cycle (v0.0.35) so some issues may be encountered.\n",
    "- Assuming that your local instance is fine, open a new tab, copy your address up until right before the first \"/lab\", and try to access the `:8090` port (i.e. `http://<...>.courses.nvidia.com:8090`). This will contain the deployment of the [**`frontend`**](frontend) folder which you can interact with.\n",
    "- You may also be interested in trying the port extension `:9012/basic_chat/playground/`, but note that most of the features there require custom handlers which are out of scope for this course.\n",
    "\n",
    "-----\n",
    "    \n",
    "**Note:** This strategy for deploying and relying on LangServe APIs within this type of environment is very non-standard and is made specifically to give students some interesting code to look at. More stable configurations are achievable with optimized single-function containers, and can be found at:\n",
    "- [**The NVIDIA \"AI Chatbot with Retrieval Augmented Generation\" Technical Brief**](https://resources.nvidia.com/en-us-generative-ai-chatbot-workflow/knowledge-base-chatbot-technical-brief)\n",
    "- [**The NVIDIA/GenerativeAIExamples GitHub Repo**](https://github.com/NVIDIA/GenerativeAIExamples/tree/main/RetrievalAugmentedGeneration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1rEA-cZWwNSx",
   "metadata": {
    "id": "1rEA-cZWwNSx"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 6:** Wrap-Up\n",
    "\n",
    "The goal of this notebook was to onboard you into the LangChain Expression Language scheme as well as provide exposure to `gradio` and `LangServe` interfaces for serving LLM functionality! There will be more of this in the subsequent notebook, but this notebook pushes towards intermediate and emerging paradigms in LLM agent development.\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Take a few minutes to look over the `frontend` directory for the deployment recipe and underlying functionality.\n",
    "2. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "3. Continue on to the next video, which will talk about **LangChain Running State Chain**.\n",
    "4. After the video, go on to the corresponding notebook on **LangChain Running State Chain**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b02f5e-6977-4c6a-b47c-7abb1d592ce4",
   "metadata": {
    "id": "b1b02f5e-6977-4c6a-b47c-7abb1d592ce4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "77c8ac2e-eb68-4b84-85fe-3a6661eba976"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
